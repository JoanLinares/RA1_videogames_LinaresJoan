# ğŸ“Š RA1 - AnÃ¡lisis de Videojuegos con Pandas y PySpark

## ğŸ“‹ Ãndice
1. [IntroducciÃ³n](#introducciÃ³n)
2. [Arquitectura del Proyecto](#arquitectura-del-proyecto)
3. [Fase 1: ExploraciÃ³n y Limpieza con Pandas](#fase-1-exploraciÃ³n-y-limpieza-con-pandas)
4. [Fase 2: Procesamiento con PySpark](#fase-2-procesamiento-con-pyspark)
5. [Fase 3: ETL con Pandas](#fase-3-etl-con-pandas)
6. [Fase 4: ETL con PySpark y Modelo Dimensional](#fase-4-etl-con-pyspark-y-modelo-dimensional)
7. [Decisiones TÃ©cnicas y JustificaciÃ³n](#decisiones-tÃ©cnicas-y-justificaciÃ³n)
8. [EjecuciÃ³n del Proyecto](#ejecuciÃ³n-del-proyecto)

---

## ğŸ“– IntroducciÃ³n

Este proyecto implementa un proceso completo de **anÃ¡lisis de datos de videojuegos** utilizando dos enfoques complementarios:
- **Pandas**: Para procesamiento en memoria y anÃ¡lisis exploratorio
- **PySpark**: Para procesamiento distribuido y escalabilidad

El objetivo principal es realizar la **exploraciÃ³n, limpieza, transformaciÃ³n y carga (ETL)** de un dataset de videojuegos, culminando en la creaciÃ³n de un **data warehouse dimensional** almacenado en bases de datos SQLite.


## ğŸ§¹ Fase 1: ExploraciÃ³n y Limpieza con Pandas

### Objetivos
1. Cargar el dataset desde mÃºltiples rutas posibles (local/contenedor)
2. Analizar tipos de datos y detectar valores faltantes
3. Realizar limpieza exhaustiva de datos
4. Normalizar y transformar columnas

### Proceso de Limpieza

#### 1. **DetecciÃ³n de Valores Especiales**
Se identificaron y reemplazaron valores que representaban datos faltantes pero no eran reconocidos como `NaN`:
```python
specials = ['?', 'N/A', 'Unknown', 'unknown', '', ' ', 'nan', 'NaN']
df_clean = df_clean.replace(specials, np.nan)
```

**JustificaciÃ³n**: Los datasets del mundo real contienen mÃºltiples representaciones de valores faltantes que deben estandarizarse para un tratamiento consistente.

#### 2. **EliminaciÃ³n de Duplicados**
```python
df_clean = df_clean.drop_duplicates()
```

**JustificaciÃ³n**: Los duplicados pueden sesgar estadÃ­sticas y anÃ¡lisis. Se eliminan para garantizar la integridad de los datos.

#### 3. **Tratamiento de Valores Faltantes**

##### Estrategia adoptada:
- **Columnas con >60% de nulos**: Se eliminan completamente
  - **JustificaciÃ³n**: Columnas con tantos valores faltantes no aportan informaciÃ³n Ãºtil y pueden generar bias
  
- **Filas con >60% de nulos**: Se eliminan
  - **JustificaciÃ³n**: Registros incompletos que no permiten anÃ¡lisis confiable

- **ImputaciÃ³n de valores numÃ©ricos**: Se usa la **mediana**
  ```python
  for col in num_cols:
      med = df_clean[col].median()
      df_clean[col] = df_clean[col].fillna(med)
  ```
  - **JustificaciÃ³n**: La mediana es robusta ante outliers, a diferencia de la media

- **ImputaciÃ³n de valores categÃ³ricos**: Se usa la **moda** (valor mÃ¡s frecuente)
  ```python
  for col in cat_cols:
      moda = df_clean[col].mode()
      if not moda.empty:
          df_clean[col] = df_clean[col].fillna(moda.iloc[0])
  ```
  - **JustificaciÃ³n**: La moda mantiene la distribuciÃ³n original de las categorÃ­as

#### 4. **NormalizaciÃ³n de Texto**
```python
# Limpieza de espacios en blanco
df_clean[col] = df_clean[col].astype(str).str.strip()

# UnificaciÃ³n de plataformas
platform_map = {
    'ps': 'PS', 'playstation': 'PS', 'ps4': 'PS', 'ps5': 'PS',
    'xbox': 'Xbox', 'xbox one': 'Xbox', ...
}
```

**JustificaciÃ³n**: 
- Elimina inconsistencias en la entrada de datos
- Reduce la cardinalidad de variables categÃ³ricas
- Facilita agrupaciones y anÃ¡lisis posteriores

#### 5. **TransformaciÃ³n de Columnas NumÃ©ricas**

Se crearon funciones especializadas para parsear diferentes formatos:

##### `parse_cost()`: Precios de videojuegos
- Convierte: `'$59.99'`, `'â‚¬49,99'`, `'free'` â†’ valores numÃ©ricos
- Maneja mÃºltiples divisas y formatos de separadores
- **JustificaciÃ³n**: Los precios vienen en formatos diversos que necesitan estandarizaciÃ³n

##### `parse_score()`: Puntuaciones
- Normaliza escalas: `'8.5/10'` â†’ `85.0`, `'47.7'` â†’ `47.7`
- **JustificaciÃ³n**: Las puntuaciones pueden estar en escala 0-10 o 0-100; se unifican a 0-100

##### `parse_millions()`: Ventas e ingresos
- Convierte: `'10M'` â†’ `10.0`, `'1.5B'` â†’ `1500.0` (millones)
- **JustificaciÃ³n**: Estandariza unidades para cÃ¡lculos matemÃ¡ticos correctos

#### 6. **Escalado de Variables NumÃ©ricas**
```python
df_clean[col + '_scaled'] = (df_clean[col] - col_min) / (col_max - col_min)
```

**JustificaciÃ³n**: 
- Normaliza valores a rango [0, 1]
- Facilita comparaciones entre variables de diferentes escalas
- Prepara datos para posibles modelos de machine learning

### Resultado Final
- **0 valores NaN** en el dataset limpio
- Datos completamente normalizados y listos para anÃ¡lisis
- Nuevas columnas calculadas con informaciÃ³n derivada

---

## âš¡ Fase 2: Procesamiento con PySpark

### Objetivos
1. Crear SparkSession para procesamiento distribuido
2. Aplicar transformaciones sobre grandes volÃºmenes de datos
3. Demostrar capacidades de agregaciÃ³n y joins

### Transformaciones Aplicadas

#### 1. **SelecciÃ³n y Filtrado**
```python
df_filtered = (
    df_numeric
    .filter(col("metascore_num").isNotNull())
    .filter(col("copies_sold_millions_num").isNotNull())
)
```

**JustificaciÃ³n**: Se filtran registros con valores nulos en mÃ©tricas clave para garantizar anÃ¡lisis vÃ¡lidos.

#### 2. **AgregaciÃ³n por GÃ©nero**
```python
df_by_genre = (
    df_filtered
    .groupBy("genre")
    .agg(
        count("*").alias("num_juegos"),
        avg("metascore_num").alias("metascore_medio"),
        sum("copies_sold_millions_num").alias("ventas_totales")
    )
)
```

**JustificaciÃ³n**: 
- Identifica gÃ©neros mÃ¡s populares y rentables
- Permite anÃ¡lisis de tendencias por categorÃ­a
- Ãštil para decisiones de negocio

#### 3. **CreaciÃ³n de Columnas Calculadas**
```python
df_with_new_cols = df_filtered.withColumn(
    "categoria_ventas",
    when(col("copies_sold_millions_num") >= 5, "Alto")
    .when(col("copies_sold_millions_num") >= 1, "Medio")
    .otherwise("Bajo")
)
```

**JustificaciÃ³n**: 
- Categoriza juegos por rendimiento comercial
- Facilita segmentaciÃ³n y anÃ¡lisis comparativo
- Proporciona insights de negocio claros

#### 4. **Join y AnÃ¡lisis Cruzado**
```python
df_joined = (
    df_with_new_cols
    .join(df_platform_stats, on="platform", how="left")
)
```

**JustificaciÃ³n**: 
- Enriquece cada registro con estadÃ­sticas agregadas de su plataforma
- Permite comparar rendimiento individual vs. promedio de plataforma
- Demuestra capacidad de PySpark para operaciones complejas

---

## ğŸ”„ Fase 3: ETL con Pandas

### Proceso ETL

#### **ExtracciÃ³n (E)**
- Se reutiliza el DataFrame limpio de la Fase 1
- Datos ya validados y normalizados

#### **TransformaciÃ³n (T)**

##### Nuevas Columnas Calculadas:
1. **`score_promedio`**: Promedio entre metascore y user_score
   - **JustificaciÃ³n**: Combina opiniÃ³n de crÃ­ticos y usuarios

2. **`categoria_ventas`**: ClasificaciÃ³n de rendimiento comercial
   - Bajo: < 1M copias
   - Moderado: 1-5M copias
   - Exitoso: 5-10M copias
   - Blockbuster: > 10M copias
   - **JustificaciÃ³n**: SegmentaciÃ³n clara para anÃ¡lisis de negocio

3. **`ingreso_por_copia`**: Revenue / Copias vendidas
   - **JustificaciÃ³n**: MÃ©trica de monetizaciÃ³n efectiva

##### Agregaciones Creadas:
- **`by_genre`**: EstadÃ­sticas por gÃ©nero
- **`by_platform`**: EstadÃ­sticas por plataforma

#### **Carga (L)**
```python
df_etl.to_sql('videogames', conn, if_exists='replace', index=False)
df_by_genre.to_sql('by_genre', conn, if_exists='replace', index=False)
df_by_platform.to_sql('by_platform', conn, if_exists='replace', index=False)
```

**Resultado**: Base de datos `warehouse_pandas.db` con 3 tablas

---

## ğŸŒŸ Fase 4: ETL con PySpark y Modelo Dimensional

### Modelo Dimensional (Esquema Estrella)

#### ğŸ¯ **DiseÃ±o del Modelo**

```
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   dim_genre     â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚ genre_id (PK)   â”‚â—„â”€â”€â”€â”€â”€â”
            â”‚ genre           â”‚      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                                     â”‚
                                     â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚      fact_videogames                â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚ fact_id (PK)                        â”‚
            â”‚ genre_id (FK)                       â”‚â”€â”€â”˜
            â”‚ platform_id (FK)                    â”‚â”€â”€â”
            â”‚ name                                â”‚  â”‚
            â”‚ metascore_num                       â”‚  â”‚
            â”‚ copies_sold_millions_num            â”‚  â”‚
            â”‚ categoria_ventas                    â”‚  â”‚
            â”‚ categoria_calidad                   â”‚  â”‚
            â”‚ num_juegos_plataforma               â”‚  â”‚
            â”‚ metascore_medio_plataforma          â”‚  â”‚
            â”‚ ventas_totales_plataforma           â”‚  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                     â”‚               â”‚
                                     â”‚               â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚               â”‚
            â”‚  dim_platform   â”‚     â”‚               â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”‚               â”‚
            â”‚ platform_id (PK)â”‚â—„â”€â”€â”€â”€â”˜               â”‚
            â”‚ platform        â”‚                     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
```

### ğŸ“Š JustificaciÃ³n del Modelo Dimensional

#### **Â¿Por quÃ© un Esquema Estrella?**

1. **Simplicidad de Consultas**
   - Las queries son mÃ¡s intuitivas y rÃ¡pidas
   - Menos JOINs necesarios para anÃ¡lisis
   - Ideal para herramientas de BI

2. **Rendimiento Optimizado**
   - DesnormalizaciÃ³n controlada reduce JOINs
   - Ãndices eficientes en claves forÃ¡neas
   - Consultas analÃ­ticas mÃ¡s rÃ¡pidas

3. **Escalabilidad**
   - FÃ¡cil agregar nuevas dimensiones
   - Tablas independientes facilitan mantenimiento
   - Crecimiento lineal de datos

### ğŸ“ Decisiones sobre Dimensiones

#### **DimensiÃ³n 1: `dim_genre` (GÃ©nero)**

**Campos:**
- `genre_id`: Clave primaria auto-generada
- `genre`: Nombre del gÃ©nero

**JustificaciÃ³n:**
- **Alto poder analÃ­tico**: Los gÃ©neros son fundamentales en la industria del videojuego
- **Baja cardinalidad**: ~10-20 gÃ©neros Ãºnicos (Action, RPG, Sports, etc.)
- **Estabilidad**: Los gÃ©neros no cambian frecuentemente
- **Casos de uso**:
  - AnÃ¡lisis de tendencias por gÃ©nero
  - ComparaciÃ³n de rendimiento entre categorÃ­as
  - IdentificaciÃ³n de gÃ©neros mÃ¡s rentables

#### **DimensiÃ³n 2: `dim_platform` (Plataforma)**

**Campos:**
- `platform_id`: Clave primaria auto-generada
- `platform`: Nombre de la plataforma

**JustificaciÃ³n:**
- **Relevancia comercial**: Las plataformas definen mercados y estrategias
- **Cardinalidad media**: ~15-30 plataformas (PS, Xbox, PC, Switch, etc.)
- **Impacto en ventas**: Diferentes plataformas tienen diferentes bases de usuarios
- **Casos de uso**:
  - AnÃ¡lisis de market share por plataforma
  - ComparaciÃ³n de rendimiento multiplataforma
  - Estrategias de lanzamiento exclusivo vs. multiplataforma

#### **Â¿Por quÃ© NO se incluyeron otras dimensiones?**

##### `dim_publisher` (Editorial) - NO incluida
- **Alta cardinalidad**: Cientos de publishers Ãºnicos
- **Menor impacto analÃ­tico** en este contexto
- **Complejidad innecesaria** para el alcance del proyecto

##### `dim_tiempo` (Fecha de lanzamiento) - NO incluida
- **Datos incompletos**: Muchos registros sin fecha precisa
- **AnÃ¡lisis temporal**: RequerirÃ­a granularidad (aÃ±o, mes, trimestre) que no es prioritaria
- **Posible extensiÃ³n futura**

### ğŸ² Tabla de Hechos: `fact_videogames`

**MÃ©tricas (Measures):**
- `metascore_num`: PuntuaciÃ³n crÃ­tica
- `copies_sold_millions_num`: Volumen de ventas
- `num_juegos_plataforma`: Contexto de la plataforma
- `metascore_medio_plataforma`: Benchmark de plataforma
- `ventas_totales_plataforma`: Potencial de mercado

**Dimensiones (Foreign Keys):**
- `genre_id`: Enlace a dim_genre
- `platform_id`: Enlace a dim_platform

**Atributos Descriptivos:**
- `name`: Nombre del juego
- `categoria_ventas`: SegmentaciÃ³n comercial
- `categoria_calidad`: SegmentaciÃ³n por calidad

**JustificaciÃ³n del DiseÃ±o:**
- **Granularidad**: Un registro por juego (nivel mÃ¡s atÃ³mico)
- **DesnormalizaciÃ³n controlada**: Se incluyen estadÃ­sticas agregadas de plataforma para evitar re-cÃ¡lculos frecuentes
- **Balance**: Combina datos transaccionales (ventas) con mÃ©tricas derivadas (categorÃ­as)

### ğŸ” Ventajas del Modelo Implementado

1. **Consultas Eficientes**
   ```sql
   -- Ejemplo: Ventas totales por gÃ©nero y plataforma
   SELECT 
       g.genre,
       p.platform,
       SUM(f.copies_sold_millions_num) as ventas_totales
   FROM fact_videogames f
   JOIN dim_genre g ON f.genre_id = g.genre_id
   JOIN dim_platform p ON f.platform_id = p.platform_id
   GROUP BY g.genre, p.platform;
   ```

2. **Mantenimiento Simplificado**
   - Actualizar un gÃ©nero afecta solo a `dim_genre`
   - Agregar nueva plataforma no altera estructura existente

3. **Escalabilidad**
   - Millones de registros en `fact_videogames` con performance Ã³ptima
   - Dimensiones pequeÃ±as caben en memoria/cachÃ©

4. **Extensibilidad**
   - FÃ¡cil agregar `dim_tiempo` en el futuro
   - Posible incluir `dim_publisher` si la cardinalidad se controla

---

## ğŸ› ï¸ Decisiones TÃ©cnicas y JustificaciÃ³n

### TecnologÃ­as Elegidas

#### **Pandas**
âœ… **Ventajas:**
- Sintaxis intuitiva y Pythonic
- Excelente para datasets de tamaÃ±o medio (<10GB)
- Amplio ecosistema de visualizaciÃ³n (Matplotlib, Seaborn)
- IntegraciÃ³n nativa con SQLite

âŒ **Limitaciones:**
- Procesamiento en memoria (limitado por RAM)
- No apto para datasets >100GB

**Uso en el proyecto**: ExploraciÃ³n inicial, limpieza exhaustiva, anÃ¡lisis exploratorio

#### **PySpark**
âœ… **Ventajas:**
- Procesamiento distribuido (escala a terabytes)
- Lazy evaluation (optimizaciÃ³n automÃ¡tica)
- API similar a SQL y Pandas
- Tolerancia a fallos

âŒ **Limitaciones:**
- Mayor overhead para datasets pequeÃ±os
- Curva de aprendizaje mÃ¡s pronunciada

**Uso en el proyecto**: Transformaciones complejas, agregaciones masivas, preparaciÃ³n para producciÃ³n

### Estrategias de Limpieza

| Problema | SoluciÃ³n Adoptada | Alternativa Descartada | JustificaciÃ³n |
|----------|-------------------|------------------------|---------------|
| Valores faltantes en columnas clave | ImputaciÃ³n con mediana/moda | EliminaciÃ³n de filas | Preserva el 80-90% de los datos |
| Columnas con >60% nulos | EliminaciÃ³n completa | ImputaciÃ³n avanzada (ML) | Coste-beneficio: demasiado esfuerzo para informaciÃ³n limitada |
| Duplicados exactos | EliminaciÃ³n | DeduplicaciÃ³n parcial | Los duplicados exactos son claramente errores de carga |
| Formatos inconsistentes | Parsing especializado | Regex genÃ©rico | Mayor precisiÃ³n y control sobre casos edge |
| Escalas diferentes | Min-Max normalization | Standardization (Z-score) | Rango [0,1] es mÃ¡s interpretable |

### Arquitectura de Datos

```
RAW DATA (CSV)
     â†“
[LIMPIEZA Y NORMALIZACIÃ“N]
     â†“
CLEAN DATA (DataFrame)
     â†“
[TRANSFORMACIONES ETL]
     â†“
MODELO DIMENSIONAL
     â†“
WAREHOUSE (SQLite)
     â†“
[CONSULTAS ANALÃTICAS]
```

---

## ğŸš€ EjecuciÃ³n del Proyecto

### Prerequisitos
- Docker y Docker Compose instalados
- Python 3.9+
- Jupyter Notebook

### OpciÃ³n 1: EjecuciÃ³n con Docker

```bash
# Levantar servicios
docker-compose up -d

# Acceder a Jupyter
# Abrir navegador en: http://localhost:8888
```

### OpciÃ³n 2: EjecuciÃ³n Local

```bash
# Instalar dependencias
pip install pandas numpy pyspark scikit-learn jupyter

# Ejecutar notebooks
jupyter notebook notebooks/01_pandas.ipynb
jupyter notebook notebooks/02_pyspark.ipynb
```

### VerificaciÃ³n de Resultados

```python
import sqlite3
import pandas as pd

# Verificar warehouse Pandas
conn = sqlite3.connect('warehouse/warehouse_pandas.db')
print(pd.read_sql("SELECT name FROM sqlite_master WHERE type='table'", conn))

# Verificar warehouse PySpark
conn_spark = sqlite3.connect('warehouse/warehouse_pyspark.db')
print(pd.read_sql("SELECT name FROM sqlite_master WHERE type='table'", conn_spark))
```

---

## ğŸ“Š Resultados Obtenidos

### MÃ©tricas de Calidad de Datos

| MÃ©trica | Antes de Limpieza | DespuÃ©s de Limpieza |
|---------|-------------------|---------------------|
| Valores nulos | ~15-30% | 0% |
| Duplicados | ~2-5% | 0% |
| Formatos inconsistentes | MÃºltiples | Estandarizados |
| Columnas eliminadas | 0 | Columnas con >60% nulos |

### Estructura Final de Bases de Datos

#### `warehouse_pandas.db`
- `videogames`: Tabla principal con todos los datos transformados
- `by_genre`: Agregaciones por gÃ©nero
- `by_platform`: Agregaciones por plataforma

#### `warehouse_pyspark.db`
- `dim_genre`: DimensiÃ³n de gÃ©neros (10-20 registros)
- `dim_platform`: DimensiÃ³n de plataformas (15-30 registros)
- `fact_videogames`: Tabla de hechos (miles de registros)

---

## ğŸ¯ Conclusiones

### Logros del Proyecto

1. âœ… **Limpieza exhaustiva**: 100% de datos vÃ¡lidos y normalizados
2. âœ… **Dos enfoques complementarios**: Pandas (anÃ¡lisis) + PySpark (escalabilidad)
3. âœ… **Modelo dimensional optimizado**: Esquema estrella con 2 dimensiones y 1 tabla de hechos
4. âœ… **ETL completo**: ExtracciÃ³n, transformaciÃ³n y carga en data warehouse
5. âœ… **DocumentaciÃ³n detallada**: JustificaciÃ³n de cada decisiÃ³n tÃ©cnica

### Lecciones Aprendidas

1. **Limpieza de datos es el 80% del trabajo**: La mayor parte del esfuerzo se invirtiÃ³ en entender y limpiar los datos
2. **La imputaciÃ³n inteligente preserva informaciÃ³n**: Usar mediana/moda es mejor que eliminar registros
3. **El modelo dimensional simplifica anÃ¡lisis**: Aunque requiere mÃ¡s diseÃ±o inicial, las consultas son mucho mÃ¡s simples
4. **PySpark brilla en agregaciones complejas**: Para transformaciones masivas, PySpark supera ampliamente a Pandas

### Posibles Extensiones Futuras

1. **AÃ±adir `dim_tiempo`**: Para anÃ¡lisis de tendencias temporales
2. **Implementar `dim_publisher`**: Si se controla la cardinalidad con agrupaciones
3. **Visualizaciones interactivas**: Dashboard con Plotly/Dash
4. **Machine Learning**: Modelos predictivos de Ã©xito comercial
5. **Pipeline automatizado**: Airflow/Prefect para ETL recurrente

---

## ğŸ‘¤ Autor

**Joan Linares**  
Proyecto: RA1 - AnÃ¡lisis de Videojuegos  
Fecha: Diciembre 2025

---

## ğŸ“š Referencias

- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
- [Kimball's Data Warehouse Toolkit](https://www.kimballgroup.com/)
- [SQLite Documentation](https://www.sqlite.org/docs.html)
